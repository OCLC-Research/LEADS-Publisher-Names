by.y = c("id", "isbn")
)
if (y == "unweighted") {
g <- graph.data.frame(network_file, directed=FALSE)
bipartite.mapping(g)
V(g)$type <- bipartite_mapping(g)$type
if (x == "walktrap") {
wc = cluster_walktrap(g)
mod = membership(wc)
} else if (x == "louvain") {
wc = cluster_louvain(g)
mod = membership(wc)
} else if (x == "infomap") {
wc = cluster_infomap(g)
mod = membership(wc)
}
} else {
g <- graph.data.frame(network_file2, directed=FALSE)
bipartite.mapping(g)
V(g)$type <- bipartite_mapping(g)$type
if (x == "walktrap") {
wc = cluster_walktrap(g)
mod = membership(wc)
} else if (x == "louvain") {
wc = cluster_louvain(g)
mod = membership(wc)
} else if (x == "infomap") {
wc = cluster_infomap(g, e.weights = network_file2$weight)
mod = membership(wc)
}
}
write.csv2(as.numeric(mod), file = filename(x, y))
data = read.csv(filename(x, y), stringsAsFactors = F, sep = ";")
return(data)
}
files = list.files("./processed_data_1m_cluster/")
raw = read.csv("./processed_data_1m/publisher_1m_p1.csv", stringsAsFactors = F)
cluster1 = read.csv("./processed_data_1m/cluster_result_single.csv", stringsAsFactors = F)
cluster = merge(
x = cluster1[, c(1, 3)], y = cluster2[, c(1, 3)],
by = "text",
all = T
)
network_file1 = read.csv("./processed_data_1m/isbn_df_simple.csv", stringsAsFactors = F)
network_file1 = network_file1[network_file1$type == 1,]
network_file1 = network_file1[, c(1:3)]
for (i in 1:3) {
x = c("walktrap", "louvain", "infomap")[i]
for (j in 1) {
y = c("unweighted")[j]
print(filename(x, y))
data = clustering(x, y)
data = data[1:max(network_file1$cluster_id),]
colnames(data)[1] = "node"
colnames(data)[2] = strsplit(filename(x, y), ".csv")[[1]][1]
data$node = data$node - 1
cluster1 = merge(
x = cluster1, y = data,
by.x = "id", by.y = "node",
all = T
)
}
}
write.csv(cluster1, "cluster_result1.csv", row.names = F)
View(cluster1)
setwd("~/Documents/GitHub/LEADS-Publisher-Names/leads-2019")
library(igraph)
# x being the algorithm
# y being whether weighted or not
# z being whether considering the relationship between publishers or not "Y" or "N"
filename = function(x, y) {
if (y == "unweighted") {
string1 = "unweighted"
} else {
string1 = "weighted"
}
filename = paste("./processed_data_1m/", x, "-", string1, ".csv", sep = "")
return(filename)
}
clustering = function(x, y) {
data = read.csv("./processed_data_1m/cluster_result_single.csv",
stringsAsFactors = F)
network_file = data.frame()
for (i in 1:nrow(data)) {
id = data$id[i]
isbns = strsplit(data$isbn_new[i], ";")[[1]]
network_file = rbind(
network_file,
data.frame(
id = id,
isbn = isbns,
stringsAsFactors = F
)
)
}
network_file1 = read.csv("./processed_data_1m/isbn_df_simple.csv", stringsAsFactors = F)
network_file1 = network_file1[network_file1$type == 1,]
network_file1 = network_file1[, c(1:3)]
colnames(network_file1) = c("id", "isbn", "weight")
network_file2 = merge(
network_file, network_file1,
by.x = c("id", "isbn"),
by.y = c("id", "isbn")
)
if (y == "unweighted") {
g <- graph.data.frame(network_file, directed=FALSE)
bipartite.mapping(g)
V(g)$type <- bipartite_mapping(g)$type
if (x == "walktrap") {
wc = cluster_walktrap(g)
mod = membership(wc)
} else if (x == "louvain") {
wc = cluster_louvain(g)
mod = membership(wc)
} else if (x == "infomap") {
wc = cluster_infomap(g)
mod = membership(wc)
}
} else {
g <- graph.data.frame(network_file2, directed=FALSE)
bipartite.mapping(g)
V(g)$type <- bipartite_mapping(g)$type
if (x == "walktrap") {
wc = cluster_walktrap(g)
mod = membership(wc)
} else if (x == "louvain") {
wc = cluster_louvain(g)
mod = membership(wc)
} else if (x == "infomap") {
wc = cluster_infomap(g, e.weights = network_file2$weight)
mod = membership(wc)
}
}
write.csv2(as.numeric(mod), file = filename(x, y))
data = read.csv(filename(x, y), stringsAsFactors = F, sep = ";")
return(data)
}
files = list.files("./processed_data_1m_cluster/")
raw = read.csv("./processed_data_1m/publisher_1m_p1.csv", stringsAsFactors = F)
cluster1 = read.csv("./processed_data_1m/cluster_result_single.csv", stringsAsFactors = F)
cluster = merge(
x = cluster1[, c(1, 3)], y = cluster2[, c(1, 3)],
by = "text",
all = T
)
network_file1 = read.csv("./processed_data_1m/isbn_df_simple.csv", stringsAsFactors = F)
network_file1 = network_file1[network_file1$type == 1,]
network_file1 = network_file1[, c(1:3)]
for (i in 1:3) {
x = c("walktrap", "louvain", "infomap")[i]
for (j in 1) {
y = c("unweighted")[j]
print(filename(x, y))
data = clustering(x, y)
data = data[1:max(network_file1$cluster_id),]
colnames(data)[1] = "node"
colnames(data)[2] = strsplit(filename(x, y), ".csv")[[1]][1]
data$node = data$node - 1
cluster1 = merge(
x = cluster1, y = data,
by.x = "id", by.y = "node",
all = T
)
}
}
write.csv(cluster1, "cluster_result1.csv", row.names = F)
setwd("~/Documents/GitHub/LEADS-Publisher-Names/leads-2019")
library(igraph)
# x being the algorithm
# y being whether weighted or not
# z being whether considering the relationship between publishers or not "Y" or "N"
filename = function(x, y) {
if (y == "unweighted") {
string1 = "unweighted"
} else {
string1 = "weighted"
}
filename = paste("./processed_data_1m/", x, "-", string1, ".csv", sep = "")
return(filename)
}
clustering = function(x, y) {
data = read.csv("./processed_data_1m/cluster_result_single.csv",
stringsAsFactors = F)
network_file = data.frame()
for (i in 1:nrow(data)) {
id = data$id[i]
isbns = strsplit(data$isbn_new[i], ";")[[1]]
network_file = rbind(
network_file,
data.frame(
id = id,
isbn = isbns,
stringsAsFactors = F
)
)
}
network_file1 = read.csv("./processed_data_1m/isbn_df_simple.csv", stringsAsFactors = F)
network_file1 = network_file1[network_file1$type == 1,]
network_file1 = network_file1[, c(1:3)]
colnames(network_file1) = c("id", "isbn", "weight")
network_file2 = merge(
network_file, network_file1,
by.x = c("id", "isbn"),
by.y = c("id", "isbn")
)
if (y == "unweighted") {
g <- graph.data.frame(network_file, directed=FALSE)
bipartite.mapping(g)
V(g)$type <- bipartite_mapping(g)$type
if (x == "walktrap") {
wc = cluster_walktrap(g)
mod = membership(wc)
} else if (x == "louvain") {
wc = cluster_louvain(g)
mod = membership(wc)
} else if (x == "infomap") {
wc = cluster_infomap(g)
mod = membership(wc)
}
} else {
g <- graph.data.frame(network_file2, directed=FALSE)
bipartite.mapping(g)
V(g)$type <- bipartite_mapping(g)$type
if (x == "walktrap") {
wc = cluster_walktrap(g)
mod = membership(wc)
} else if (x == "louvain") {
wc = cluster_louvain(g)
mod = membership(wc)
} else if (x == "infomap") {
wc = cluster_infomap(g, e.weights = network_file2$weight)
mod = membership(wc)
}
}
write.csv2(as.numeric(mod), file = filename(x, y))
data = read.csv(filename(x, y), stringsAsFactors = F, sep = ";")
return(data)
}
files = list.files("./processed_data_1m_cluster/")
raw = read.csv("./processed_data_1m/publisher_1m_p1.csv", stringsAsFactors = F)
cluster1 = read.csv("./processed_data_1m/cluster_result_single.csv", stringsAsFactors = F)
network_file1 = read.csv("./processed_data_1m/isbn_df_simple.csv", stringsAsFactors = F)
network_file1 = network_file1[network_file1$type == 1,]
network_file1 = network_file1[, c(1:3)]
for (i in 1:3) {
x = c("walktrap", "louvain", "infomap")[i]
for (j in 1) {
y = c("unweighted")[j]
print(filename(x, y))
data = clustering(x, y)
data = data[1:max(network_file1$cluster_id),]
colnames(data)[1] = "node"
colnames(data)[2] = strsplit(filename(x, y), ".csv")[[1]][1]
data$node = data$node - 1
cluster1 = merge(
x = cluster1, y = data,
by.x = "id", by.y = "node",
all = T
)
}
}
write.csv(cluster1, "cluster_result1.csv", row.names = F)
import pandas as pd
import re
import numpy as np
def common_remove(str):
str = re.sub(r"[^a-zA-Z0-9' ]+", ' ', str).strip()
str = re.sub(r" +", ' ', str).strip()
str = re.sub(r"[0-9]{4}$|[0-9] {4}$", "", str).strip()
return(str)
file2 = "./processed_data_1m/isbn_df_simple.csv"
data = pd.read_csv("./processed_data_1m/cluster_result_single.csv")
raw = pd.read_csv("./processed_data_1m/publisher_1m_p1.csv")
raw = raw.loc[raw['F008_0710'] == "2010"]
raw_new = raw[pd.notnull(raw['isbn_tag'])]
raw_new = raw_new.reset_index(drop=True)
raw_pro = []
for i in range(len(raw_new.index)):
isbn_text = raw_new.loc[i, 'isbn_tag']
isbn_vector = isbn_text.split(" ;; ")
if len(isbn_vector) > 1:
for j in range(len(isbn_vector)):
row = {}
row["ID"] = raw_new.loc[i, 'F001_a']
row["ISBN"] = isbn_vector[j]
row["Publisher"] = raw_new.loc[i, 'F26x_b_cleaned']
raw_pro.append(row)
else:
row = {}
row["ID"] = raw_new.loc[i, 'F001_a']
row["ISBN"] = isbn_vector[0]
row["Publisher"] = raw_new.loc[i, 'F26x_b_cleaned']
raw_pro.append(row)
raw_pro_df = pd.DataFrame(raw_pro)
raw_pro_df.to_csv("abc.csv")
isbn_df = []
for i in range(len(data.index)):
print(i)
isbn_vector = data.loc[i, 'isbn_new']
text_vector = data.loc[i, 'publisher_pro']
isbn_list = isbn_vector.split(";")
text_list = text_vector.split(";;")
for k in range(len(isbn_list)):
count = 0
row = {}
row['isbn'] = isbn_list[k]
row['cluster_id'] = data.loc[i, 'id']
for j in range(len(text_list)):
df_sub = raw_pro_df[raw_pro_df['Publisher'].str.contains(common_remove(text_list[j]), case=False)]
df_sub = df_sub[df_sub['ISBN'] == isbn_list[k]]
count = count + len(df_sub.index)
row["record"] = count
row['type'] = "1"
isbn_df.append(row)
isbn_df = pd.DataFrame(isbn_df)
isbn_df = isbn_df.replace({'record': {0: 1}})
isbn_df.to_csv(file2, index_label=False, index=False)
raw_pro_df.to_csv("./processed_data_1m/ID_cluster.csv", index=False, index_label=False)
setwd("~/Documents/GitHub/LEADS-Publisher-Names/leads-2019")
library(igraph)
# x being the algorithm
# y being whether weighted or not
# z being whether considering the relationship between publishers or not "Y" or "N"
filename = function(x, y) {
if (y == "unweighted") {
string1 = "unweighted"
} else {
string1 = "weighted"
}
filename = paste("./processed_data_1m/", x, "-", string1, ".csv", sep = "")
return(filename)
}
clustering = function(x, y) {
data = read.csv("./processed_data_1m/cluster_result_single.csv",
stringsAsFactors = F)
network_file = data.frame()
for (i in 1:nrow(data)) {
id = data$id[i]
isbns = strsplit(data$isbn_new[i], ";")[[1]]
network_file = rbind(
network_file,
data.frame(
id = id,
isbn = isbns,
stringsAsFactors = F
)
)
}
network_file1 = read.csv("./processed_data_1m/isbn_df_simple.csv", stringsAsFactors = F)
network_file1 = network_file1[network_file1$type == 1,]
network_file1 = network_file1[, c(1:3)]
colnames(network_file1) = c("id", "isbn", "weight")
network_file2 = merge(
network_file, network_file1,
by.x = c("id", "isbn"),
by.y = c("id", "isbn")
)
if (y == "unweighted") {
g <- graph.data.frame(network_file, directed=FALSE)
bipartite.mapping(g)
V(g)$type <- bipartite_mapping(g)$type
if (x == "walktrap") {
wc = cluster_walktrap(g)
mod = membership(wc)
} else if (x == "louvain") {
wc = cluster_louvain(g)
mod = membership(wc)
} else if (x == "infomap") {
wc = cluster_infomap(g)
mod = membership(wc)
}
} else {
g <- graph.data.frame(network_file2, directed=FALSE)
bipartite.mapping(g)
V(g)$type <- bipartite_mapping(g)$type
if (x == "walktrap") {
wc = cluster_walktrap(g)
mod = membership(wc)
} else if (x == "louvain") {
wc = cluster_louvain(g)
mod = membership(wc)
} else if (x == "infomap") {
wc = cluster_infomap(g, e.weights = network_file2$weight)
mod = membership(wc)
}
}
write.csv2(as.numeric(mod), file = filename(x, y))
data = read.csv(filename(x, y), stringsAsFactors = F, sep = ";")
return(data)
}
files = list.files("./processed_data_1m_cluster/")
raw = read.csv("./processed_data_1m/publisher_1m_p1.csv", stringsAsFactors = F)
cluster1 = read.csv("./processed_data_1m/cluster_result_single.csv", stringsAsFactors = F)
network_file1 = read.csv("./processed_data_1m/isbn_df_simple.csv", stringsAsFactors = F)
network_file1 = network_file1[network_file1$type == 1,]
network_file1 = network_file1[, c(1:3)]
for (i in 1:3) {
x = c("walktrap", "louvain", "infomap")[i]
for (j in 1) {
y = c("unweighted")[j]
print(filename(x, y))
data = clustering(x, y)
data = data[1:max(network_file1$cluster_id),]
colnames(data)[1] = "node"
colnames(data)[2] = strsplit(filename(x, y), ".csv")[[1]][1]
data$node = data$node - 1
cluster1 = merge(
x = cluster1, y = data,
by.x = "id", by.y = "node",
all = T
)
}
}
write.csv(cluster1, "cluster_result1.csv", row.names = F)
setwd("~/Documents/GitHub/LEADS-Publisher-Names/leads-2019")
library(igraph)
# x being the algorithm
# y being whether weighted or not
# z being whether considering the relationship between publishers or not "Y" or "N"
filename = function(x, y) {
if (y == "unweighted") {
string1 = "unweighted"
} else {
string1 = "weighted"
}
filename = paste("./processed_data_1m/", x, "-", string1, ".csv", sep = "")
return(filename)
}
clustering = function(x, y) {
data = read.csv("./processed_data_1m/cluster_result_single.csv",
stringsAsFactors = F)
network_file = data.frame()
for (i in 1:nrow(data)) {
id = data$id[i]
isbns = strsplit(data$isbn_new[i], ";")[[1]]
network_file = rbind(
network_file,
data.frame(
id = id,
isbn = isbns,
stringsAsFactors = F
)
)
}
network_file1 = read.csv("./processed_data_1m/isbn_df_simple.csv", stringsAsFactors = F)
network_file1 = network_file1[network_file1$type == 1,]
network_file1 = network_file1[, c(1:3)]
colnames(network_file1) = c("id", "isbn", "weight")
network_file2 = merge(
network_file, network_file1,
by.x = c("id", "isbn"),
by.y = c("id", "isbn")
)
if (y == "unweighted") {
g <- graph.data.frame(network_file, directed=FALSE)
bipartite.mapping(g)
V(g)$type <- bipartite_mapping(g)$type
if (x == "walktrap") {
wc = cluster_walktrap(g)
mod = membership(wc)
} else if (x == "louvain") {
wc = cluster_louvain(g)
mod = membership(wc)
} else if (x == "infomap") {
wc = cluster_infomap(g)
mod = membership(wc)
}
} else {
g <- graph.data.frame(network_file2, directed=FALSE)
bipartite.mapping(g)
V(g)$type <- bipartite_mapping(g)$type
if (x == "walktrap") {
wc = cluster_walktrap(g)
mod = membership(wc)
} else if (x == "louvain") {
wc = cluster_louvain(g)
mod = membership(wc)
} else if (x == "infomap") {
wc = cluster_infomap(g, e.weights = network_file2$weight)
mod = membership(wc)
}
}
write.csv2(as.numeric(mod), file = filename(x, y))
data = read.csv(filename(x, y), stringsAsFactors = F, sep = ";")
return(data)
}
files = list.files("./processed_data_1m_cluster/")
raw = read.csv("./processed_data_1m/publisher_1m_p1.csv", stringsAsFactors = F)
cluster1 = read.csv("./processed_data_1m/cluster_result_single.csv", stringsAsFactors = F)
network_file1 = read.csv("./processed_data_1m/isbn_df_simple.csv", stringsAsFactors = F)
network_file1 = network_file1[network_file1$type == 1,]
network_file1 = network_file1[, c(1:3)]
for (i in 1:3) {
x = c("walktrap", "louvain", "infomap")[i]
for (j in 1) {
y = c("unweighted")[j]
print(filename(x, y))
data = clustering(x, y)
data = data[1:max(network_file1$cluster_id),]
colnames(data)[1] = "node"
colnames(data)[2] = strsplit(filename(x, y), ".csv")[[1]][1]
data$node = data$node - 1
cluster1 = merge(
x = cluster1, y = data,
by.x = "id", by.y = "node",
all = T
)
}
}
write.csv(cluster1, "cluster_result1.csv", row.names = F)
install.packages("slowraker")
citation("slowraker")
